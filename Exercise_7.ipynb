{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5yOkGTUF6Kp"
   },
   "source": [
    "# Recap\n",
    "\n",
    "First we need access to data. \n",
    "- You can use this link to add the data to your drive: https://drive.google.com/drive/folders/1pHNxZVrlcKh5usWoNC_V7gR2WdeDutjv\n",
    "\n",
    "- Then go inside the folder **CS_for_MedStudents_data** and you will see the folder **HAM10000**.\n",
    "- Right click on the **HAM10000** folder and click on the **Add to my Drive** option.\n",
    "\n",
    "Now you can run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L4ugGnx7F6Kw",
    "outputId": "812f6010-a6bf-4cb4-b238-96a3e5e3d452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rtwesOsZF6K6"
   },
   "outputs": [],
   "source": [
    "# Check the directory\n",
    "data_dir = \"/content/drive/My Drive/HAM10000\"\n",
    "\n",
    "classes = [ 'actinic keratoses', 'basal cell carcinoma', 'benign keratosis-like lesions', \n",
    "           'dermatofibroma','melanoma', 'melanocytic nevi', 'vascular lesions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjF-Aq9BF6K_"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Imagenet values\n",
    "norm_mean = (0.4914, 0.4822, 0.4465)\n",
    "norm_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# define the transformaitons the images go through each time it is used for training\n",
    "# includes augmentation AND normalization as descirbed above\n",
    "augmentation_train = transforms.Compose([\n",
    "                                  # resize image to the network input size\n",
    "                                  transforms.Resize((224,224)),\n",
    "                                  # randomly perform a horizontal flip of the image\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  # rotate the image with a angle from 0 to 60 (chosen randomly)\n",
    "                                  transforms.RandomRotation(degrees=60),\n",
    "                                  # convert the image into a tensor so it can be processed by the GPU\n",
    "                                  transforms.ToTensor(),\n",
    "                                  # normalize the image with the mean and std of ImageNet\n",
    "                                  transforms.Normalize(norm_mean, norm_std),\n",
    "                                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yS7bZZuOF6LE"
   },
   "outputs": [],
   "source": [
    "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
    "augmentation_test = transforms.Compose([\n",
    "                    transforms.Resize((224,224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v194TjXLF6LH"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# create an instance of the image folder class to load images by classes defined with the folders given\n",
    "dataset = torchvision.datasets.ImageFolder(root= data_dir, transform= augmentation_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jB0u2jdsF6LK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get the total amount of images in the dataset\n",
    "num_train = len(dataset)\n",
    "\n",
    "# create a list of indices for the whole dataset\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# get the class labels from the dataset object (0-6)\n",
    "class_labels = dataset.targets\n",
    "\n",
    "# define the percentage of data that is not used for training\n",
    "split_size = 0.2\n",
    "\n",
    "# call a function of sklarn that takes care of splitting the dataset into training and validation+testing\n",
    "train_indices, test_indices, class_labels_train, class_labels_test = train_test_split(indices,\n",
    "                                                                                       class_labels,\n",
    "                                                                                       test_size=split_size,\n",
    "                                                                                       shuffle=True,\n",
    "                                                                                       stratify= class_labels,\n",
    "                                                                                       random_state=42)\n",
    "\n",
    "# call a function of sklearn that splits validation+training into validation and training\n",
    "train_indices, val_indices = train_test_split(train_indices,\n",
    "                                               test_size=split_size,\n",
    "                                               shuffle=True,\n",
    "                                               stratify= class_labels_train,\n",
    "                                               random_state=42)\n",
    "\n",
    "# Creating data samplers and loaders using the indices:\n",
    "SubsetRandomSampler = torch.utils.data.sampler.SubsetRandomSampler\n",
    "\n",
    "# create instances of a torch class for picking random samples from our dataset\n",
    "train_samples = SubsetRandomSampler(train_indices)\n",
    "val_samples = SubsetRandomSampler(val_indices)\n",
    "test_samples = SubsetRandomSampler(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LW-_scRF6LQ"
   },
   "outputs": [],
   "source": [
    "# define the batch size for training, val and testing\n",
    "batch_size, validation_batch_size, test_batch_size = 32, 32, 32\n",
    "\n",
    "# create and instance of a dataloader for training\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False,num_workers=1, sampler= train_samples)\n",
    "\n",
    "# overwrite the dataset instance with the test augmentation (this is not nice code)\n",
    "dataset = torchvision.datasets.ImageFolder(root= data_dir, transform=augmentation_test)\n",
    "# create instances of a dataloaders for validation and testing\n",
    "validation_data_loader = torch.utils.data.DataLoader(dataset, batch_size=validation_batch_size, shuffle=False, sampler=val_samples)\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset, batch_size=test_batch_size, shuffle=False, sampler=test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJr1S0byF6LU"
   },
   "source": [
    "# Define a Convolutional Neural Network\n",
    "\n",
    "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from torch library.\n",
    "\n",
    "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authorsâ€™ implementation of LeNet was used primarily for OCR and character recognition in documents.\n",
    "\n",
    "The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foGNkuwzF6LV"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = len(classes)\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, (5,5), padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, (5,5)) \n",
    "        self.fc1   = nn.Linear(16*54*54, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = LeNet()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TE_R_SSWF6Lb"
   },
   "source": [
    "# Define a Loss function\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss.\n",
    "\n",
    "$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$\n",
    "\n",
    "### Median Frequency Balancing\n",
    "In Exercise 6 you had seen that the number of samples per classes in the HAM10000 is not equal. \n",
    "\n",
    "![Class Distribution](https://github.com/IFL-CAMP/ML_for_MedStudents/blob/master/Images/class_dist.png?raw=true)\n",
    "\n",
    "So, if we train using this data, our model will be biased towards the class  with more samples, i.e nv or melanocytic nevi. Therefore, we need to counter this class imbalance in our dataset. As a solution, we use **Median Frequency Balancing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "vCJn-DV5F6Lc",
    "outputId": "5165c2b1-aba2-4c09-d430-0249709c549e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actinic keratoses : 1.5718654434250765\n",
      "basal cell carcinoma : 1.0\n",
      "benign keratosis-like lesions : 0.467697907188353\n",
      "dermatofibroma : 4.469565217391304\n",
      "melanoma : 0.4618149146451033\n",
      "melanocytic nevi : 0.07665920954511558\n",
      "vascular lesions : 3.619718309859155\n"
     ]
    }
   ],
   "source": [
    "# Median Frequency Balancing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# get the class labels of each image\n",
    "class_labels = dataset.targets\n",
    "# empty array for counting instance of each class\n",
    "count_labels = np.zeros(len(classes))\n",
    "# empty array for weights of each class\n",
    "class_weights = np.zeros(len(classes))\n",
    "\n",
    "# populate the count array\n",
    "for l in class_labels:\n",
    "  count_labels[l] += 1\n",
    "\n",
    "# get median count\n",
    "median_freq = np.median(count_labels)\n",
    "\n",
    "# calculate the weigths\n",
    "for i in range(len(classes)):\n",
    "  class_weights[i] = median_freq/count_labels[i]\n",
    "\n",
    "# print the weights\n",
    "for i in range(len(classes)):\n",
    "    print(classes[i],\":\", class_weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pPUGMq3NCnP"
   },
   "source": [
    "Now we define the loss function with the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Q72RJ-M84P"
   },
   "outputs": [],
   "source": [
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRBdC31sF6Li"
   },
   "source": [
    "# Define the Optimizer\n",
    "\n",
    "The most common and effective Optimizer currently used is **Adam: Adaptive Moments**. You can look [here](https://arxiv.org/abs/1412.6980) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "pmMjYnCjF6Lk",
    "outputId": "d8e41c96-74e6-4c01-b8dd-0c61ad17494a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=46656, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oGr-AfqF6Ln"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "zyc3or5oF6Lo",
    "outputId": "fe8982bd-1602-4ed4-9cec-d0d369c03365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 1.5936264689288921\n",
      "Epoch: 2\n",
      "Loss: 1.571835981079595\n",
      "Epoch: 3\n",
      "Loss: 1.5633033460645533\n",
      "Epoch: 4\n",
      "Loss: 1.5391058927744776\n",
      "Epoch: 5\n",
      "Loss: 1.5206250545397326\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "accuracy = []\n",
    "val_accuracy = []\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    \n",
    "    running_loss /= len(train_data_loader)\n",
    "\n",
    "    print('Epoch: {}'.format(epoch+1))\n",
    "    print('Loss: {}' .format(running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "LwjnpkM5HDBO",
    "outputId": "860ac81d-7727-40f1-9707-343513c5120c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 224, 224]             456\n",
      "            Conv2d-2         [-1, 16, 108, 108]           2,416\n",
      "            Linear-3                  [-1, 120]       5,598,840\n",
      "            Linear-4                   [-1, 84]          10,164\n",
      "            Linear-5                    [-1, 7]             595\n",
      "================================================================\n",
      "Total params: 5,612,471\n",
      "Trainable params: 5,612,471\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.72\n",
      "Params size (MB): 21.41\n",
      "Estimated Total Size (MB): 25.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpDIh3GWHMiT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Exercise_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

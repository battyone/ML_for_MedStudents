{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_Statistics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RefQbhODWEe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "y_true = torch.tensor([0,1,2,2,3,3,3,4,4,4,4,5,5,5,5,5])\n",
        "y_pred = torch.tensor([1,1,3,3,1,2,3,4,4,1,1,1,2,3,4,5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftV-zFGGWVEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_cm_and_class_stats(y_true, y_pred, verbose=False):\n",
        "  # get list of class labels by calling the unique vlaues in y_pred see https://pytorch.org/docs/stable/torch.html#torch.unique \n",
        "  print(y_true)\n",
        "  print(y_pred)\n",
        "  classes = y_true.unique()\n",
        "\n",
        "  # number of classes\n",
        "  num_classes = len(classes)\n",
        "\n",
        "  # initialiye confusion matrix see https://en.wikipedia.org/wiki/Confusion_matrix\n",
        "  conf_matrix = torch.zeros(num_classes, num_classes)\n",
        "\n",
        "  # map just puts the two tensors together so they can be iterated over at once think of stacking them together\n",
        "  # add the value in the corresponding confusion matrix cell by one for each truth / predicitn pair\n",
        "  for t, p in zip(y_true, y_pred):\n",
        "    conf_matrix[t, p] += 1\n",
        "\n",
        "  # let's print the confusion matrix\n",
        "  print('\\nConfusion Matrix')\n",
        "  dim_0_name = 'Ground Truth'.center(2 * num_classes, ' ')\n",
        "  dim_1_name = 'Prediction'.center(4 * num_classes, ' ')\n",
        "  print(dim_1_name)\n",
        "  for i in range(num_classes):\n",
        "    print(dim_0_name[2*i], end = '')\n",
        "    for j in range(num_classes):\n",
        "      print (f'{conf_matrix[i][j].item():4.0f}', end = '')\n",
        "    print('\\n'+dim_0_name[2*i+1])\n",
        "\n",
        "  # the diagonal represents the correctly predicted samples for each class\n",
        "  # these are the true positive samples\n",
        "  true_positives = conf_matrix.diag()\n",
        "  if verbose:\n",
        "    print('\\ntrue positives per class:',true_positives)\n",
        "\n",
        "  # all predicted positives are calculated by summing up all predictions for each class\n",
        "  # they include true positive and false positive samples\n",
        "  predicted_positives = torch.sum(conf_matrix, dim=0)\n",
        "  if verbose:\n",
        "    print('all positives predictions per class:',predicted_positives)\n",
        "\n",
        "  # false positives = samples where class was predicted but actually is another class\n",
        "  # we get this by subtracting the correctly positive classified samples from all positive predictions\n",
        "  false_positives = predicted_positives - true_positives\n",
        "  if verbose:\n",
        "    print('false positives per class:',false_positives)\n",
        "\n",
        "  # false negatives = samples which where not classified correctly (as positive)\n",
        "  # i.e. the actual positive samples minus the ones detected (true positives)\n",
        "  # therefore we need the actual positive samples per class first\n",
        "  actual_positives = torch.sum(conf_matrix, dim=1)\n",
        "  if verbose:\n",
        "    print('actual positives per class:',actual_positives)\n",
        "\n",
        "  # false_negatives = all actual positive samples per class minus the ones that were predicted correctly\n",
        "  false_negatives = actual_positives - true_positives\n",
        "  if verbose:\n",
        "    print('false negatives per class:',false_negatives)\n",
        "\n",
        "  # true negatives = predicted_negatives - false negatives\n",
        "  # therefore we first have to get all negatives by subtracting all positive samples per class from the total number of samples\n",
        "  all_samples = conf_matrix.sum()\n",
        "  if verbose:\n",
        "    print('total number of samples:',all_samples)\n",
        "\n",
        "  # note: all samples is just a scalar number, but will be deflated to the size of all_positives (i.e. the number of classes)\n",
        "  predicted_negatives = all_samples - predicted_positives\n",
        "  if verbose:\n",
        "    print('all negative predictions per class:',predicted_negatives)\n",
        "\n",
        "  # now we can calculate the true negatives\n",
        "  true_negatives = predicted_negatives - false_negatives\n",
        "  if verbose:\n",
        "    print('true negatives per class:',true_negatives)\\\n",
        "\n",
        "  # for faster typing, let's rename those\n",
        "  fn = false_negatives\n",
        "  fp = false_positives\n",
        "  tn = true_negatives\n",
        "  tp = true_positives\n",
        "  actual_negatives = tn + fp\n",
        "\n",
        "  assert(fn + fp +  tn + tp == all_samples).all, 'sanity check failed'\n",
        "\n",
        "  #--------------------\n",
        "  # OVERALL STATISTICS\n",
        "  #--------------------\n",
        "  print('\\nGlobal statistics:')\n",
        "  \n",
        "  # calculate accuracy by dividing correct predictions by total number of predictions\n",
        "  accuracy = (y_pred == y_true).sum().float()/len(y_pred)\n",
        "  print(f'- Accuracy: {accuracy:.2%}')\n",
        "\n",
        "\n",
        "  #--------------------\n",
        "  # CLASS STATISTICS\n",
        "  #--------------------\n",
        "  # finally we can calculate class wise metrics:\n",
        "  class_stat={}\n",
        "\n",
        "  # Sensitivity = TP / (TP+FN) = TP / P \n",
        "  # https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "  # also called the true positive rate, the recall, or probability of detection\n",
        "  # measures the proportion of actual positives that are correctly identified as such \n",
        "  # e.g., the percentage of sick people who are correctly identified as having the condition\n",
        "  class_stat['Sensitivity'] = tp / actual_positives\n",
        "  recall = class_stat['Sensitivity']\n",
        "\n",
        "  # Specificity = TN / (TN+FP) = TN / N \n",
        "  # https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
        "  # also called the true negative rate\n",
        "  # measures the proportion of actual negatives that are correctly identified as such\n",
        "  # e.g., the percentage of healthy people who are correctly identified as not having the condition\n",
        "  class_stat['Specificity'] = tn / actual_negatives\n",
        "\n",
        "  # Accuracy = (TP + TN) / ALL = (TP + TN) / P + N = (TP + TN) / (TP+FN+TN+FP)\n",
        "  # https://en.wikipedia.org/wiki/Accuracy_and_precision\n",
        "  # measure of a test's accuracy\n",
        "  class_stat['Accuracy'] = (tp+tn) / (actual_positives + actual_negatives)\n",
        "  #class_ind = 0\n",
        "  #print(f\"TP: {tp[class_ind]}; TN: {tn[class_ind]}; positives: {actual_positives[class_ind]}, negatives: {actual_negatives[class_ind]}\")\n",
        "\n",
        "  # Precision = TP / (TP+FP)\n",
        "  # https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "  # also called positive predictive value\n",
        "  # fraction of relevant instances among the retrieved instances\n",
        "  # (for recall = sensitivity see above)\n",
        "  class_stat['Precision'] = tp / predicted_positives\n",
        "  precision = class_stat['Precision']\n",
        "\n",
        "  # F1-Score = 2 * (recall*precision)/(recall+precision)\n",
        "  # https://en.wikipedia.org/wiki/F1_score\n",
        "  # measure of a test's accuracy\n",
        "  class_stat['F1-Score'] = 2 * recall * precision / (recall+precision)\n",
        "\n",
        "  print('\\nStatistics per class:')\n",
        "  print_row('Classes', classes, head=True)\n",
        "  for key, stat in class_stat.items():\n",
        "    print_row(key, stat)\n",
        "\n",
        "def print_row(name, items, col_width = 12, head=False):\n",
        "  num_cells = len(items)\n",
        "  if head:\n",
        "    print((('_'*col_width)+'_')*(num_cells+1))\n",
        "  print(((' '*col_width)+'|')*(num_cells+1))\n",
        "  print(name.ljust(col_width)+'|', end='')\n",
        "  for cell in items:\n",
        "    print(f'{cell:^{col_width}{\".0f\" if head else \".2%\"}}|', end='')\n",
        "  print('\\n'+(('_'*col_width)+'|')*(num_cells+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYgHqIoydWaK",
        "colab_type": "code",
        "outputId": "98eb0afd-d550-4b5a-c18c-39fdf182a0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "source": [
        "# you can set verbose=True for more details\n",
        "print_cm_and_class_stats(y_true=y_true, y_pred=y_pred)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5])\n",
            "tensor([1, 1, 3, 3, 1, 2, 3, 4, 4, 1, 1, 1, 2, 3, 4, 5])\n",
            "\n",
            "Confusion Matrix\n",
            "       Prediction       \n",
            "G   0   1   0   0   0   0\n",
            "r\n",
            "o   0   1   0   0   0   0\n",
            "u\n",
            "n   0   0   0   2   0   0\n",
            "d\n",
            "    0   1   1   1   0   0\n",
            "T\n",
            "r   0   2   0   0   2   0\n",
            "u\n",
            "t   0   1   1   1   1   1\n",
            "h\n",
            "\n",
            "Global statistics:\n",
            "- Accuracy: 31.25%\n",
            "\n",
            "Statistics per class:\n",
            "___________________________________________________________________________________________\n",
            "            |            |            |            |            |            |            |\n",
            "Classes     |     0      |     1      |     2      |     3      |     4      |     5      |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n",
            "            |            |            |            |            |            |            |\n",
            "Sensitivity |   0.00%    |  100.00%   |   0.00%    |   33.33%   |   50.00%   |   20.00%   |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n",
            "            |            |            |            |            |            |            |\n",
            "Specificity |  100.00%   |   66.67%   |   85.71%   |   76.92%   |   91.67%   |  100.00%   |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n",
            "            |            |            |            |            |            |            |\n",
            "Accuracy    |   93.75%   |   68.75%   |   75.00%   |   68.75%   |   81.25%   |   75.00%   |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n",
            "            |            |            |            |            |            |            |\n",
            "Precision   |    nan%    |   16.67%   |   0.00%    |   25.00%   |   66.67%   |  100.00%   |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n",
            "            |            |            |            |            |            |            |\n",
            "F1-Score    |    nan%    |   28.57%   |    nan%    |   28.57%   |   57.14%   |   33.33%   |\n",
            "____________|____________|____________|____________|____________|____________|____________|\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDdl9zrp4BrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}